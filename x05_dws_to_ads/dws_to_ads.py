"""
DWSåˆ°ADSå±‚åº”ç”¨æ•°æ®æœåŠ¡
ä»DWSæ±‡æ€»è¡¨æ„å»ºæœ€ç»ˆæŒ‡æ ‡è¡¨å¹¶å­˜å…¥MySQL
"""
import logging
from tools.spark_util import SparkUtil
from config.config_manager import ConfigManager
from pyspark.sql.functions import *
from pyspark.sql.types import *

class DWSToADS:
    def __init__(self):
        self.spark = SparkUtil.get_spark_session("DWS-To-ADS")
        self.config_manager = ConfigManager()
        self.paths_config = self.config_manager.get_paths_config()
    
    def build_ads_trade_stats_by_tm(self):
        """æ„å»ºå„å“ç‰Œå•†å“äº¤æ˜“ç»Ÿè®¡"""
        try:
            print("\nå¼€å§‹æ„å»ºå„å“ç‰Œå•†å“äº¤æ˜“ç»Ÿè®¡...")
            
            # è¯»å–DWSå•†å“ç²’åº¦è®¢å•æ±‡æ€»è¡¨
            dws_trade_sku_order_path = f"{self.paths_config['dws_path']}/dws_trade_sku_order_nd"
            dws_trade_sku_order_df = SparkUtil.read_from_hdfs(dws_trade_sku_order_path, spark=self.spark)
            
            # è¯»å–å•†å“ç»´åº¦è¡¨
            dim_sku_path = f"{self.paths_config['dim_path']}/dim_sku"
            dim_sku_df = SparkUtil.read_from_hdfs(dim_sku_path, spark=self.spark)
            
            # å…³è”ç»´åº¦è¡¨å¹¶æŒ‰å“ç‰Œç»Ÿè®¡
            ads_trade_stats_by_tm = dws_trade_sku_order_df.alias("dws") \
                .join(dim_sku_df.alias("sku"), col("dws.sku_id") == col("sku.sku_id"), "inner") \
                .groupBy("sku.tm_id", "sku.tm_name") \
                .agg(
                    sum("dws.order_count").alias("order_count"),
                    sum("dws.user_count").alias("user_count"),
                    sum("dws.order_num").alias("order_num"),
                    sum("dws.order_amount").alias("order_amount"),
                    sum("dws.activity_reduce_amount").alias("activity_reduce_amount"),
                    sum("dws.coupon_reduce_amount").alias("coupon_reduce_amount")
                ) \
                .select(
                    col("tm_id"),
                    col("tm_name"),
                    col("order_count"),
                    col("user_count"),
                    col("order_num"),
                    col("order_amount"),
                    col("activity_reduce_amount"),
                    col("coupon_reduce_amount"),
                    current_date().alias("dt")
                )
            
            # å†™å…¥MySQL
            SparkUtil.write_to_mysql(ads_trade_stats_by_tm, "ads_trade_stats_by_tm", spark=self.spark)
            
            print(f"âœ“ å„å“ç‰Œå•†å“äº¤æ˜“ç»Ÿè®¡æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_trade_stats_by_tm.count()}")
            
        except Exception as e:
            print(f"âœ— å„å“ç‰Œå•†å“äº¤æ˜“ç»Ÿè®¡æ„å»ºå¤±è´¥: {e}")
            raise
    
    def build_ads_trade_stats_by_cate(self):
        """æ„å»ºå„åˆ†ç±»å•†å“äº¤æ˜“ç»Ÿè®¡"""
        try:
            print("\nå¼€å§‹æ„å»ºå„åˆ†ç±»å•†å“äº¤æ˜“ç»Ÿè®¡...")
            
            # è¯»å–DWSå•†å“ç²’åº¦è®¢å•æ±‡æ€»è¡¨
            dws_trade_sku_order_path = f"{self.paths_config['dws_path']}/dws_trade_sku_order_nd"
            dws_trade_sku_order_df = SparkUtil.read_from_hdfs(dws_trade_sku_order_path, spark=self.spark)
            
            # è¯»å–å•†å“ç»´åº¦è¡¨
            dim_sku_path = f"{self.paths_config['dim_path']}/dim_sku"
            dim_sku_df = SparkUtil.read_from_hdfs(dim_sku_path, spark=self.spark)
            
            # å…³è”ç»´åº¦è¡¨å¹¶æŒ‰ä¸€çº§åˆ†ç±»ç»Ÿè®¡
            ads_trade_stats_by_cate1 = dws_trade_sku_order_df.alias("dws") \
                .join(dim_sku_df.alias("sku"), col("dws.sku_id") == col("sku.sku_id"), "inner") \
                .groupBy("sku.category1_id", "sku.category1_name") \
                .agg(
                    sum("dws.order_count").alias("order_count"),
                    sum("dws.user_count").alias("user_count"),
                    sum("dws.order_num").alias("order_num"),
                    sum("dws.order_amount").alias("order_amount"),
                    sum("dws.activity_reduce_amount").alias("activity_reduce_amount"),
                    sum("dws.coupon_reduce_amount").alias("coupon_reduce_amount")
                ) \
                .select(
                    col("category1_id").alias("category_id"),
                    col("category1_name").alias("category_name"),
                    lit(1).alias("category_level"),
                    col("order_count"),
                    col("user_count"),
                    col("order_num"),
                    col("order_amount"),
                    col("activity_reduce_amount"),
                    col("coupon_reduce_amount"),
                    current_date().alias("dt")
                )
            
            # å†™å…¥MySQL
            SparkUtil.write_to_mysql(ads_trade_stats_by_cate1, "ads_trade_stats_by_cate", spark=self.spark)
            
            print(f"âœ“ å„åˆ†ç±»å•†å“äº¤æ˜“ç»Ÿè®¡æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_trade_stats_by_cate1.count()}")
            
        except Exception as e:
            print(f"âœ— å„åˆ†ç±»å•†å“äº¤æ˜“ç»Ÿè®¡æ„å»ºå¤±è´¥: {e}")
            raise
    
    def build_ads_trade_stats_by_province(self):
        """æ„å»ºå„çœä»½äº¤æ˜“ç»Ÿè®¡"""
        try:
            print("\nå¼€å§‹æ„å»ºå„çœä»½äº¤æ˜“ç»Ÿè®¡...")
            
            # è¯»å–DWSåœ°åŒºç²’åº¦è®¢å•æ±‡æ€»è¡¨
            dws_trade_province_order_path = f"{self.paths_config['dws_path']}/dws_trade_province_order_nd"
            dws_trade_province_order_df = SparkUtil.read_from_hdfs(dws_trade_province_order_path, spark=self.spark)
            
            # è¯»å–çœä»½è¡¨ï¼ˆæ­£ç¡®çš„çœä»½ä¿¡æ¯æ¥æºï¼‰
            base_province_path = f"{self.paths_config['ods_path']}/base_province"
            base_province_df = SparkUtil.read_from_hdfs(base_province_path, spark=self.spark)
            
            print(f"DWSåœ°åŒºç²’åº¦è¡¨è®°å½•æ•°: {dws_trade_province_order_df.count()}")
            print(f"çœä»½è¡¨è®°å½•æ•°: {base_province_df.count()}")
            
            # æ˜¾ç¤ºæ•°æ®æ ·æœ¬è¿›è¡Œè°ƒè¯•
            print("\nDWSåœ°åŒºç²’åº¦è¡¨æ•°æ®æ ·æœ¬:")
            dws_trade_province_order_df.show(10, truncate=False)
            
            print("\nçœä»½è¡¨æ•°æ®æ ·æœ¬:")
            base_province_df.select("id", "name", "region_id", "area_code", "iso_code", "iso_3166_2").show(10, truncate=False)
            
            # æ£€æŸ¥province_idåœ¨ä¸¤ä¸ªè¡¨ä¸­çš„åˆ†å¸ƒ
            print("\nDWSè¡¨ä¸­province_idåˆ†å¸ƒ:")
            dws_province_ids = dws_trade_province_order_df.select("province_id").distinct()
            dws_province_ids.show(20, truncate=False)
            
            print("\nçœä»½è¡¨ä¸­idåˆ†å¸ƒ:")
            base_province_ids = base_province_df.select("id").distinct()
            base_province_ids.show(20, truncate=False)
            
            # å…³è”çœä»½ä¿¡æ¯ - åŒ…å«æ–°å¢çš„iso_codeå’Œiso_3166_2å­—æ®µ
            ads_trade_stats_by_province = dws_trade_province_order_df.alias("dws") \
                .join(base_province_df.alias("bp"), 
                      col("dws.province_id") == col("bp.id"), "inner") \
                .select(
                    col("dws.province_id"),
                    col("bp.name").alias("province_name"),
                    col("bp.region_id"),
                    col("bp.area_code"),                    
                    col("bp.iso_code"),                     
                    col("bp.iso_3166_2"),                   
                    col("dws.order_count"),
                    col("dws.user_count"),
                    col("dws.order_num"),
                    col("dws.order_amount"),
                    col("dws.activity_reduce_amount"),
                    col("dws.coupon_reduce_amount"),
                    current_date().alias("dt")
                )
            
            print(f"\nå…³è”åè®°å½•æ•°: {ads_trade_stats_by_province.count()}")
            
            if ads_trade_stats_by_province.count() > 0:
                print("\nå…³è”æˆåŠŸï¼Œæ˜¾ç¤ºç»Ÿè®¡ç»“æœ:")
                ads_trade_stats_by_province.orderBy(col("order_amount").desc()).show(20, truncate=False)
                
                # å†™å…¥MySQL
                SparkUtil.write_to_mysql(ads_trade_stats_by_province, "ads_trade_stats_by_province", spark=self.spark)
                
                print(f"âœ“ å„çœä»½äº¤æ˜“ç»Ÿè®¡æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_trade_stats_by_province.count()}")
            else:
                print("âš ï¸ å…³è”åæ²¡æœ‰æ•°æ®ï¼Œå°è¯•ç±»å‹è½¬æ¢å…³è”...")
                
                # å°è¯•ç±»å‹è½¬æ¢å…³è”
                ads_trade_stats_by_province_converted = dws_trade_province_order_df.alias("dws") \
                    .join(base_province_df.alias("bp"), 
                          col("dws.province_id").cast("string") == col("bp.id").cast("string"), "inner") \
                    .select(
                        col("dws.province_id"),
                        col("bp.name").alias("province_name"),
                        col("bp.region_id"),
                        col("bp.area_code"),                    # æ–°å¢ï¼šè¡Œæ”¿åŒºä½ç 
                        col("bp.iso_code"),                     # æ–°å¢ï¼šå›½é™…ç¼–ç 
                        col("bp.iso_3166_2"),                   # æ–°å¢ï¼šISO3166ç¼–ç 
                        col("dws.order_count"),
                        col("dws.user_count"),
                        col("dws.order_num"),
                        col("dws.order_amount"),
                        col("dws.activity_reduce_amount"),
                        col("dws.coupon_reduce_amount"),
                        current_date().alias("dt")
                    )
                
                print(f"ç±»å‹è½¬æ¢åè®°å½•æ•°: {ads_trade_stats_by_province_converted.count()}")
                
                if ads_trade_stats_by_province_converted.count() > 0:
                    print("\nç±»å‹è½¬æ¢åå…³è”æˆåŠŸ:")
                    ads_trade_stats_by_province_converted.orderBy(col("order_amount").desc()).show(20, truncate=False)
                    
                    # å†™å…¥MySQL
                    SparkUtil.write_to_mysql(ads_trade_stats_by_province_converted, "ads_trade_stats_by_province", spark=self.spark)
                    
                    print(f"âœ“ å„çœä»½äº¤æ˜“ç»Ÿè®¡æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_trade_stats_by_province_converted.count()}")
                else:
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨LEFT JOINæŸ¥çœ‹è¯¦æƒ…
                    print("è¿›ä¸€æ­¥è°ƒè¯• - ä½¿ç”¨LEFT JOIN:")
                    debug_join = dws_trade_province_order_df.alias("dws") \
                        .join(base_province_df.alias("bp"), 
                              col("dws.province_id").cast("string") == col("bp.id").cast("string"), "left") \
                        .select(
                            col("dws.province_id"),
                            col("bp.id").alias("base_province_id"),
                            col("bp.name").alias("province_name"),
                            col("bp.area_code"),
                            col("bp.iso_code"),
                            col("bp.iso_3166_2"),
                            col("dws.order_count")
                        )
                    
                    print("LEFT JOINè°ƒè¯•ç»“æœ:")
                    debug_join.show(20, truncate=False)
                    
                    print("âŒ æ— æ³•äº§ç”Ÿæœ‰æ•ˆçš„çœä»½äº¤æ˜“ç»Ÿè®¡æ•°æ®")
            
        except Exception as e:
            print(f"âœ— å„çœä»½äº¤æ˜“ç»Ÿè®¡æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
        
    def build_ads_user_stats(self):
        """æ„å»ºç”¨æˆ·ç»Ÿè®¡"""
        try:
            print("\nå¼€å§‹æ„å»ºç”¨æˆ·ç»Ÿè®¡...")
            
            # è¯»å–DWSç”¨æˆ·ç²’åº¦è®¢å•æ±‡æ€»è¡¨
            dws_trade_user_order_path = f"{self.paths_config['dws_path']}/dws_trade_user_order_nd"
            dws_trade_user_order_df = SparkUtil.read_from_hdfs(dws_trade_user_order_path, spark=self.spark)
            
            # è¯»å–DWSç”¨æˆ·ç²’åº¦ç™»å½•æ±‡æ€»è¡¨
            dws_user_user_login_path = f"{self.paths_config['dws_path']}/dws_user_user_login_nd"
            dws_user_user_login_df = SparkUtil.read_from_hdfs(dws_user_user_login_path, spark=self.spark)
            
            # æ³¨å†Œä¸´æ—¶è§†å›¾
            dws_trade_user_order_df.createOrReplaceTempView("dws_trade_user_order")
            dws_user_user_login_df.createOrReplaceTempView("dws_user_user_login")
            
            # ä½¿ç”¨SQLæ„å»ºç»Ÿè®¡ç»“æœ
            ads_user_stats = self.spark.sql("""
                SELECT 
                    metric_name,
                    CAST(metric_value AS DOUBLE) as metric_value,
                    current_date() as dt
                FROM (
                    SELECT 'total_user_count' as metric_name, COUNT(*) as metric_value
                    FROM dws_user_user_login
                    
                    UNION ALL
                    
                    SELECT 'order_user_count' as metric_name, COUNT(*) as metric_value
                    FROM dws_trade_user_order
                    
                    UNION ALL
                    
                    SELECT 'total_order_count' as metric_name, COALESCE(SUM(order_count), 0) as metric_value
                    FROM dws_trade_user_order
                    
                    UNION ALL
                    
                    SELECT 'total_order_amount' as metric_name, COALESCE(SUM(order_amount), 0) as metric_value
                    FROM dws_trade_user_order
                ) user_stats
            """)
            
            # å†™å…¥MySQL
            SparkUtil.write_to_mysql(ads_user_stats, "ads_user_stats", spark=self.spark)
            
            print(f"âœ“ ç”¨æˆ·ç»Ÿè®¡æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_user_stats.count()}")
            
        except Exception as e:
            print(f"âœ— ç”¨æˆ·ç»Ÿè®¡æ„å»ºå¤±è´¥: {e}")
            raise
        
    def build_ads_sku_favor_count_top10(self):
        """æ„å»ºå•†å“æ”¶è—æ’è¡Œæ¦œTOP10"""
        try:
            print("\nå¼€å§‹æ„å»ºå•†å“æ”¶è—æ’è¡Œæ¦œTOP10...")
            
            # è¯»å–DWSå•†å“ç²’åº¦æ”¶è—æ±‡æ€»è¡¨
            dws_interaction_sku_favor_path = f"{self.paths_config['dws_path']}/dws_interaction_sku_favor_add_nd"
            dws_interaction_sku_favor_df = SparkUtil.read_from_hdfs(dws_interaction_sku_favor_path, spark=self.spark)
            
            # è¯»å–å•†å“ç»´åº¦è¡¨
            dim_sku_path = f"{self.paths_config['dim_path']}/dim_sku"
            dim_sku_df = SparkUtil.read_from_hdfs(dim_sku_path, spark=self.spark)
            
            # å…³è”ç»´åº¦è¡¨å¹¶å–TOP10
            ads_sku_favor_count_top10 = dws_interaction_sku_favor_df.alias("dws") \
                .join(dim_sku_df.alias("sku"), col("dws.sku_id") == col("sku.sku_id"), "inner") \
                .select(
                    col("dws.sku_id"),
                    col("sku.sku_name"),
                    col("sku.tm_name"),
                    col("sku.category1_name"),
                    col("sku.category2_name"),
                    col("sku.category3_name"),
                    col("dws.favor_count"),
                    col("dws.favor_user_count"),
                    current_date().alias("dt")
                ) \
                .orderBy(col("favor_count").desc()) \
                .limit(10)
            
            # å†™å…¥MySQL
            SparkUtil.write_to_mysql(ads_sku_favor_count_top10, "ads_sku_favor_count_top10", spark=self.spark)
            
            print(f"âœ“ å•†å“æ”¶è—æ’è¡Œæ¦œTOP10æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_sku_favor_count_top10.count()}")
            
        except Exception as e:
            print(f"âœ— å•†å“æ”¶è—æ’è¡Œæ¦œTOP10æ„å»ºå¤±è´¥: {e}")
            raise
    
    def build_ads_coupon_stats(self):
        """æ„å»ºä¼˜æƒ åˆ¸ä½¿ç”¨ç»Ÿè®¡"""
        try:
            print("\nå¼€å§‹æ„å»ºä¼˜æƒ åˆ¸ä½¿ç”¨ç»Ÿè®¡...")
            
            # è¯»å–DWSç”¨æˆ·ä¼˜æƒ åˆ¸ç²’åº¦ä¼˜æƒ åˆ¸ä½¿ç”¨æ±‡æ€»è¡¨
            dws_tool_user_coupon_path = f"{self.paths_config['dws_path']}/dws_tool_user_coupon_coupon_used_nd"
            dws_tool_user_coupon_df = SparkUtil.read_from_hdfs(dws_tool_user_coupon_path, spark=self.spark)
            
            # è¯»å–ä¼˜æƒ åˆ¸ç»´åº¦è¡¨
            dim_coupon_path = f"{self.paths_config['dim_path']}/dim_coupon"
            dim_coupon_df = SparkUtil.read_from_hdfs(dim_coupon_path, spark=self.spark)
            
            # æŒ‰ä¼˜æƒ åˆ¸ç»Ÿè®¡ä½¿ç”¨æƒ…å†µ
            ads_coupon_stats = dws_tool_user_coupon_df.alias("dws") \
                .join(dim_coupon_df.alias("coupon"), col("dws.coupon_id") == col("coupon.coupon_id"), "inner") \
                .groupBy("coupon.coupon_id", "coupon.coupon_name", "coupon.coupon_type") \
                .agg(
                    sum("dws.used_count").alias("used_count"),
                    countDistinct("dws.user_id").alias("used_user_count")
                ) \
                .select(
                    col("coupon_id"),
                    col("coupon_name"),
                    col("coupon_type"),
                    col("used_count"),
                    col("used_user_count"),
                    current_date().alias("dt")
                )
            
            # å†™å…¥MySQL
            SparkUtil.write_to_mysql(ads_coupon_stats, "ads_coupon_stats", spark=self.spark)
            
            print(f"âœ“ ä¼˜æƒ åˆ¸ä½¿ç”¨ç»Ÿè®¡æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_coupon_stats.count()}")
            
        except Exception as e:
            print(f"âœ— ä¼˜æƒ åˆ¸ä½¿ç”¨ç»Ÿè®¡æ„å»ºå¤±è´¥: {e}")
            raise

    def build_ads_user_change(self):
        """æ„å»ºç”¨æˆ·å˜åŠ¨ç»Ÿè®¡è¡¨ï¼ˆæµå¤±ç”¨æˆ·+å›æµç”¨æˆ·ï¼‰- æ”¯æŒå¤šä¸ªæ—¶é—´ç»´åº¦"""
        try:
            print("\nå¼€å§‹æ„å»ºç”¨æˆ·å˜åŠ¨ç»Ÿè®¡è¡¨...")
    
            # è·å–ç”¨æˆ·ç™»å½•æ±‡æ€»æ•°æ®
            dws_user_login_path = f"{self.paths_config['dws_path']}/dws_user_user_login_nd"
            dws_user_login_df = SparkUtil.read_from_hdfs(dws_user_login_path, spark=self.spark)
    
            print(f"ç”¨æˆ·ç™»å½•æ±‡æ€»è¡¨è®°å½•æ•°: {dws_user_login_df.count()}")
            
            # æ˜¾ç¤ºæ•°æ®ç»“æ„å’Œæ ·æœ¬
            print("\nç”¨æˆ·ç™»å½•æ±‡æ€»è¡¨å­—æ®µç»“æ„:")
            dws_user_login_df.printSchema()
            
            print("\nç”¨æˆ·ç™»å½•æ±‡æ€»è¡¨æ•°æ®æ ·æœ¬:")
            dws_user_login_df.show(5, truncate=False)
    
            # æ£€æŸ¥ç™»å½•æ—¶é—´çš„è¯¦ç»†åˆ†å¸ƒ
            print("\næœ€è¿‘ç™»å½•æ—¶é—´è¯¦ç»†åˆ†å¸ƒ:")
            time_distribution = dws_user_login_df.groupBy("last_login_time").count().orderBy("last_login_time")
            time_distribution.show(20, truncate=False)
    
            # è·å–å½“å‰æ—¥æœŸä½œä¸ºPythonå¯¹è±¡
            from datetime import date
            today = date.today()
            
            print(f"ç»Ÿè®¡åŸºå‡†æ—¥æœŸ: {today}")
    
            # æ³¨å†Œä¸´æ—¶è§†å›¾è¿›è¡ŒSQLæ“ä½œ
            dws_user_login_df.createOrReplaceTempView("user_login_summary")
    
            # å¢å¼ºçš„ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡SQL - æ”¯æŒå¤šä¸ªæ—¶é—´ç»´åº¦
            user_change_sql = """
            WITH date_calculations AS (
                SELECT 
                    user_id,
                    last_login_time,
                    datediff(current_date(), last_login_time) as days_since_last_login,
                    CASE 
                        WHEN date_format(last_login_time, 'yyyy-MM-dd') = current_date() THEN 1 
                        ELSE 0 
                    END as is_today_login,
                    CASE 
                        WHEN date_format(last_login_time, 'yyyy-MM-dd') = date_sub(current_date(), 1) THEN 1 
                        ELSE 0 
                    END as is_yesterday_login,
                    CASE 
                        WHEN datediff(current_date(), last_login_time) BETWEEN 1 AND 3 THEN 1 
                        ELSE 0 
                    END as is_recent_3days_login,
                    CASE 
                        WHEN datediff(current_date(), last_login_time) BETWEEN 1 AND 7 THEN 1 
                        ELSE 0 
                    END as is_recent_7days_login
                FROM user_login_summary
            ),
            user_stats_detailed AS (
                SELECT
                    -- ä»Šæ—¥æ´»è·ƒç”¨æˆ·
                    SUM(is_today_login) as today_active_users,
                    -- æ˜¨æ—¥æ´»è·ƒç”¨æˆ·
                    SUM(is_yesterday_login) as yesterday_active_users,
                    -- æœ€è¿‘3å¤©æ´»è·ƒç”¨æˆ·
                    SUM(is_recent_3days_login) as recent_3days_active_users,
                    -- æœ€è¿‘7å¤©æ´»è·ƒç”¨æˆ·
                    SUM(is_recent_7days_login) as recent_7days_active_users,
                    
                    -- æµå¤±ç”¨æˆ·ï¼ˆä¸åŒæ—¶é—´ç»´åº¦ï¼‰
                    SUM(CASE WHEN days_since_last_login > 3 THEN 1 ELSE 0 END) as churn_3days_count,
                    SUM(CASE WHEN days_since_last_login > 7 THEN 1 ELSE 0 END) as churn_7days_count,
                    SUM(CASE WHEN days_since_last_login > 30 THEN 1 ELSE 0 END) as churn_30days_count,
                    
                    -- å›æµç”¨æˆ·ï¼ˆç®€åŒ–å®šä¹‰ï¼šä»Šæ—¥ç™»å½•ä¸”ä¹‹å‰Nå¤©æœªç™»å½•ï¼‰
                    SUM(CASE 
                        WHEN is_today_login = 1 AND days_since_last_login >= 3 THEN 1 
                        ELSE 0 
                    END) as back_from_3days_count,
                    SUM(CASE 
                        WHEN is_today_login = 1 AND days_since_last_login >= 7 THEN 1 
                        ELSE 0 
                    END) as back_from_7days_count,
                    
                    -- æ€»ç”¨æˆ·æ•°
                    COUNT(*) as total_users
                FROM date_calculations
            )
            SELECT 
                current_date() as dt,
                'user_activity_summary' as metric_type,
                
                -- æ´»è·ƒç”¨æˆ·æŒ‡æ ‡
                today_active_users,
                yesterday_active_users,
                recent_3days_active_users,
                recent_7days_active_users,
                
                -- æµå¤±ç”¨æˆ·æŒ‡æ ‡
                churn_3days_count,
                churn_7days_count,
                churn_30days_count,
                
                -- å›æµç”¨æˆ·æŒ‡æ ‡
                back_from_3days_count,
                back_from_7days_count,
                
                -- æ€»ç”¨æˆ·æ•°
                total_users,
                
                -- æ´»è·ƒç‡è®¡ç®—
                ROUND(today_active_users * 100.0 / total_users, 2) as today_active_rate,
                ROUND(recent_7days_active_users * 100.0 / total_users, 2) as week_active_rate
            FROM user_stats_detailed
            """
    
            print("\næ‰§è¡Œå¢å¼ºçš„ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡SQL...")
            ads_user_change_detailed = self.spark.sql(user_change_sql)
    
            # æ˜¾ç¤ºè¯¦ç»†è®¡ç®—ç»“æœ
            print("\nè¯¦ç»†ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡ç»“æœ:")
            ads_user_change_detailed.show(truncate=False)
    
            # åˆ›å»ºç¬¦åˆåŸè¡¨ç»“æ„çš„ç®€åŒ–ç‰ˆæœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
            ads_user_change_simple = self.spark.sql("""
            WITH date_calculations AS (
                SELECT 
                    user_id,
                    last_login_time,
                    datediff(current_date(), last_login_time) as days_since_last_login
                FROM user_login_summary
            ),
            user_stats AS (
                SELECT
                    -- æµå¤±ç”¨æˆ·æ•°ï¼ˆ3å¤©ä»¥ä¸Šæœªç™»å½•ï¼Œå› ä¸º7å¤©æ•°æ®å¤ªå°‘ï¼‰
                    SUM(CASE WHEN days_since_last_login > 3 THEN 1 ELSE 0 END) as churn_count,
                    -- å›æµç”¨æˆ·ï¼ˆä»Šæ—¥ç™»å½•ä¸”ä¹‹å‰3å¤©æœªç™»å½•ï¼‰
                    SUM(CASE 
                        WHEN date_format(last_login_time, 'yyyy-MM-dd') = current_date() 
                             AND days_since_last_login >= 3 THEN 1 
                        ELSE 0 
                    END) as back_count
                FROM date_calculations
            )
            SELECT 
                current_date() as dt,
                CAST(COALESCE(churn_count, 0) AS INT) as user_churn_count,
                CAST(COALESCE(back_count, 0) AS INT) as user_back_count
            FROM user_stats
            """)
    
            print("\nç®€åŒ–ç‰ˆç”¨æˆ·å˜åŠ¨ç»Ÿè®¡ç»“æœ:")
            ads_user_change_simple.show(truncate=False)
    
            # å¦‚æœç®€åŒ–ç‰ˆæœ¬ä»ç„¶ä¸º0ï¼Œä½¿ç”¨æ›´å®½æ¾çš„æ¡ä»¶
            if ads_user_change_simple.collect()[0]['user_churn_count'] == 0 and ads_user_change_simple.collect()[0]['user_back_count'] == 0:
                print("âš ï¸ ä½¿ç”¨æ›´å®½æ¾çš„ç»Ÿè®¡æ¡ä»¶ï¼ˆ1å¤©é—´éš”ï¼‰...")
                ads_user_change_relaxed = self.spark.sql("""
                WITH date_calculations AS (
                    SELECT 
                        user_id,
                        last_login_time,
                        datediff(current_date(), last_login_time) as days_since_last_login
                    FROM user_login_summary
                ),
                user_stats AS (
                    SELECT
                        -- æµå¤±ç”¨æˆ·æ•°ï¼ˆ1å¤©ä»¥ä¸Šæœªç™»å½•ï¼‰
                        SUM(CASE WHEN days_since_last_login > 1 THEN 1 ELSE 0 END) as churn_count,
                        -- ä»Šæ—¥ç™»å½•ç”¨æˆ·æ•°
                        SUM(CASE 
                            WHEN date_format(last_login_time, 'yyyy-MM-dd') = current_date() THEN 1 
                            ELSE 0 
                        END) as today_login_count,
                        -- æ˜¨æ—¥ç™»å½•ç”¨æˆ·æ•°
                        SUM(CASE 
                            WHEN date_format(last_login_time, 'yyyy-MM-dd') = date_sub(current_date(), 1) THEN 1 
                            ELSE 0 
                        END) as yesterday_login_count
                    FROM date_calculations
                )
                SELECT 
                    current_date() as dt,
                    CAST(COALESCE(churn_count, 0) AS INT) as user_churn_count,
                    CAST(COALESCE(today_login_count, 0) AS INT) as user_back_count
                FROM user_stats
                """)
                
                print("\nå®½æ¾æ¡ä»¶ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡ç»“æœ:")
                ads_user_change_relaxed.show(truncate=False)
                
                ads_user_change = ads_user_change_relaxed
            else:
                ads_user_change = ads_user_change_simple
    
            # éªŒè¯æ•°æ®ç±»å‹
            print("\næœ€ç»ˆæ•°æ®ç»“æ„:")
            ads_user_change.printSchema()
            ads_user_change.show()
    
            # å†™å…¥MySQL
            SparkUtil.write_to_mysql(ads_user_change, "ads_user_change", mode="overwrite", spark=self.spark)
    
            print(f"âœ“ ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡è¡¨æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_user_change.count()}")
    
            # æ˜¾ç¤ºç»Ÿè®¡æ±‡æ€»ä¿¡æ¯
            user_change_result = ads_user_change.collect()[0]
            print(f"ğŸ“Š ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡æ±‡æ€»:")
            print(f"  ç»Ÿè®¡æ—¥æœŸ: {user_change_result['dt']}")
            print(f"  æµå¤±ç”¨æˆ·æ•°: {user_change_result['user_churn_count']}")
            print(f"  å›æµç”¨æˆ·æ•°: {user_change_result['user_back_count']}")
    
            # åŒæ—¶è¾“å‡ºè¯¦ç»†ç»Ÿè®¡åˆ°æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            if 'ads_user_change_detailed' in locals():
                print("\nğŸ“ˆ è¯¦ç»†æ´»è·ƒåº¦åˆ†æ:")
                detailed_result = ads_user_change_detailed.collect()[0]
                print(f"  æ€»ç”¨æˆ·æ•°: {detailed_result['total_users']}")
                print(f"  ä»Šæ—¥æ´»è·ƒ: {detailed_result['today_active_users']} ({detailed_result['today_active_rate']}%)")
                print(f"  æ˜¨æ—¥æ´»è·ƒ: {detailed_result['yesterday_active_users']}")
                print(f"  è¿‘7æ—¥æ´»è·ƒ: {detailed_result['recent_7days_active_users']} ({detailed_result['week_active_rate']}%)")
                print(f"  3æ—¥æµå¤±: {detailed_result['churn_3days_count']}")
                print(f"  7æ—¥æµå¤±: {detailed_result['churn_7days_count']}")
    
        except Exception as e:
            print(f"âœ— ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡è¡¨æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise 

    def build_ads_order_to_pay_interval_avg(self):
        """æ„å»ºä¸‹å•åˆ°æ”¯ä»˜æ—¶é—´é—´éš”å¹³å‡å€¼æŒ‡æ ‡è¡¨"""
        try:
            print("\nå¼€å§‹æ„å»ºä¸‹å•åˆ°æ”¯ä»˜æ—¶é—´é—´éš”å¹³å‡å€¼æŒ‡æ ‡è¡¨...")
    
            # 1. è¯»å–æ”¯ä»˜æˆåŠŸäº‹å®è¡¨
            dwd_trade_pay_detail_path = f"{self.paths_config['dwd_path']}/dwd_trade_pay_detail"
            pay_detail_df = SparkUtil.read_from_hdfs(dwd_trade_pay_detail_path, spark=self.spark)
    
            # 2. è¯»å–è®¢å•ä¿¡æ¯è¡¨(ä»ODSå±‚è¯»å–ï¼Œå› ä¸ºéœ€è¦è®¢å•çš„åŸå§‹åˆ›å»ºæ—¶é—´)
            order_info_path = f"{self.paths_config['ods_path']}/order_info"
            order_info_df = SparkUtil.read_from_hdfs(order_info_path, spark=self.spark)
    
            print(f"æ”¯ä»˜è¡¨è®°å½•æ•°: {pay_detail_df.count()}")
            print(f"è®¢å•ä¿¡æ¯è¡¨è®°å½•æ•°: {order_info_df.count()}")
    
            # æ˜¾ç¤ºæ•°æ®æ ·æœ¬è¿›è¡Œè°ƒè¯•
            print("\næ”¯ä»˜è¡¨æ—¶é—´å­—æ®µæ ·æœ¬:")
            pay_detail_df.select("order_id", "create_time", "callback_time", "dt").show(5, truncate=False)
            
            print("\nè®¢å•ä¿¡æ¯è¡¨æ—¶é—´å­—æ®µæ ·æœ¬:")
            order_info_df.select("id", "create_time", "operate_time").show(5, truncate=False)
    
            # 3. å…³è”è®¢å•å’Œæ”¯ä»˜ä¿¡æ¯ - ä¿®æ­£å…³è”é€»è¾‘
            order_pay_join = pay_detail_df.alias("pay") \
                .join(order_info_df.alias("order"),
                      col("pay.order_id") == col("order.id"),
                      "inner") \
                .select(
                    col("pay.order_id"),
                    col("order.create_time").alias("order_create_time"),
                    col("pay.callback_time").alias("pay_callback_time"),
                    col("pay.dt")
                ) \
                .filter(
                    (col("order_create_time").isNotNull()) & 
                    (col("pay_callback_time").isNotNull())
                )
    
            print(f"\nå…³è”åè®°å½•æ•°: {order_pay_join.count()}")
            
            if order_pay_join.count() > 0:
                # æ˜¾ç¤ºå…³è”åçš„æ ·æœ¬æ•°æ®
                print("\nå…³è”åæ—¶é—´å­—æ®µæ ·æœ¬:")
                order_pay_join.show(5, truncate=False)
    
                # 4. è®¡ç®—æ¯ä¸ªè®¢å•çš„ä¸‹å•åˆ°æ”¯ä»˜æ—¶é—´é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
                order_pay_interval = order_pay_join \
                    .withColumn("order_to_pay_interval_seconds",
                               unix_timestamp(col("pay_callback_time")) -
                               unix_timestamp(col("order_create_time"))) \
                    .withColumn("order_to_pay_interval",
                               col("order_to_pay_interval_seconds") / 60.0) \
                    .filter(col("order_to_pay_interval") >= 0)  # è¿‡æ»¤æ‰è´Ÿå€¼ï¼ˆå¼‚å¸¸æ•°æ®ï¼‰
    
                print("\næ—¶é—´é—´éš”è®¡ç®—ç»“æœæ ·æœ¬:")
                order_pay_interval.select(
                    "order_id", 
                    "order_create_time", 
                    "pay_callback_time", 
                    "order_to_pay_interval_seconds",
                    "order_to_pay_interval"
                ).show(10, truncate=False)
    
                # æ£€æŸ¥æ—¶é—´é—´éš”çš„åˆ†å¸ƒ
                print("\næ—¶é—´é—´éš”åˆ†å¸ƒç»Ÿè®¡:")
                interval_stats = order_pay_interval.agg(
                    count("*").alias("total_orders"),
                    min("order_to_pay_interval").alias("min_interval"),
                    max("order_to_pay_interval").alias("max_interval"),
                    avg("order_to_pay_interval").alias("avg_interval"),
                    stddev("order_to_pay_interval").alias("stddev_interval")
                )
                interval_stats.show()
    
                # 5. æŒ‰å¤©è®¡ç®—å¹³å‡æ—¶é—´é—´éš”
                ads_order_to_pay_interval_avg = order_pay_interval \
                    .groupBy("dt") \
                    .agg(
                        avg("order_to_pay_interval").alias("order_to_pay_interval_avg"),
                        count("*").alias("order_count"),
                        min("order_to_pay_interval").alias("min_interval"),
                        max("order_to_pay_interval").alias("max_interval")
                    ) \
                    .select(
                        col("dt"),
                        col("order_to_pay_interval_avg").cast("decimal(10,2)").alias("order_to_pay_interval_avg"),
                        col("order_count"),
                        col("min_interval").cast("decimal(10,2)").alias("min_interval"),
                        col("max_interval").cast("decimal(10,2)").alias("max_interval")
                    ) \
                    .orderBy("dt")
    
                print(f"\næœ€ç»ˆç»Ÿè®¡ç»“æœ:")
                ads_order_to_pay_interval_avg.show(truncate=False)
    
                if ads_order_to_pay_interval_avg.count() > 0:
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸º0çš„å€¼
                    zero_intervals = ads_order_to_pay_interval_avg.filter(col("order_to_pay_interval_avg") == 0)
                    if zero_intervals.count() > 0:
                        print("âš ï¸ ä»æœ‰æ—¶é—´é—´éš”ä¸º0çš„è®°å½•:")
                        zero_intervals.show()
    
                    # 6. å†™å…¥MySQL
                    SparkUtil.write_to_mysql(ads_order_to_pay_interval_avg, "ads_order_to_pay_interval_avg", mode="overwrite", spark=self.spark)
    
                    print(f"âœ“ ä¸‹å•åˆ°æ”¯ä»˜æ—¶é—´é—´éš”å¹³å‡å€¼æŒ‡æ ‡è¡¨æ„å»ºå®Œæˆï¼Œè®°å½•æ•°: {ads_order_to_pay_interval_avg.count()}")
                else:
                    print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»Ÿè®¡ç»“æœ")
            else:
                print("âŒ è®¢å•å’Œæ”¯ä»˜ä¿¡æ¯å…³è”å¤±è´¥ï¼Œæ²¡æœ‰åŒ¹é…çš„è®°å½•")
    
        except Exception as e:
            print(f"âœ— ä¸‹å•åˆ°æ”¯ä»˜æ—¶é—´é—´éš”å¹³å‡å€¼æŒ‡æ ‡è¡¨æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    def build_all_ads(self):
        """æ„å»ºæ‰€æœ‰ADSåº”ç”¨æ•°æ®æœåŠ¡è¡¨"""
        print("=== å¼€å§‹æ„å»ºADSåº”ç”¨æ•°æ®æœåŠ¡è¡¨ ===")

        ads_list = [
            ("å„å“ç‰Œå•†å“äº¤æ˜“ç»Ÿè®¡", self.build_ads_trade_stats_by_tm),
            ("å„åˆ†ç±»å•†å“äº¤æ˜“ç»Ÿè®¡", self.build_ads_trade_stats_by_cate),
            ("å„çœä»½äº¤æ˜“ç»Ÿè®¡", self.build_ads_trade_stats_by_province),
            ("ç”¨æˆ·ç»Ÿè®¡", self.build_ads_user_stats),
            ("ç”¨æˆ·å˜åŠ¨ç»Ÿè®¡", self.build_ads_user_change),
            ("å•†å“æ”¶è—æ’è¡Œæ¦œTOP10", self.build_ads_sku_favor_count_top10),
            ("ä¼˜æƒ åˆ¸ä½¿ç”¨ç»Ÿè®¡", self.build_ads_coupon_stats),
            ("ä¸‹å•åˆ°æ”¯ä»˜æ—¶é—´é—´éš”å¹³å‡å€¼", self.build_ads_order_to_pay_interval_avg)  # æ–°å¢æŒ‡æ ‡
        ]

        success_count = 0
        failure_count = 0

        for ads_name, ads_func in ads_list:
            try:
                ads_func()
                success_count += 1
            except Exception as e:
                failure_count += 1
                print(f"âœ— {ads_name} æ„å»ºå¤±è´¥: {e}")
                continue

        print(f"\n=== ADSå±‚æ„å»ºå®Œæˆ ===")
        print(f"æˆåŠŸ: {success_count}å¼ åº”ç”¨è¡¨")
        print(f"å¤±è´¥: {failure_count}å¼ åº”ç”¨è¡¨")
        print(f"æ€»è®¡: {len(ads_list)}å¼ åº”ç”¨è¡¨")

def main():
    """ä¸»å‡½æ•°"""
    try:
        dws_to_ads = DWSToADS()
        dws_to_ads.build_all_ads()
        
    except Exception as e:
        logging.error(f"DWSåˆ°ADSæ„å»ºå¤±è´¥: {e}")
        print(f"âœ— DWSåˆ°ADSæ„å»ºå¤±è´¥: {e}")
    finally:
        SparkUtil.stop_spark_session()

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, 
                       format='%(asctime)s - %(levelname)s - %(message)s')
    main()
